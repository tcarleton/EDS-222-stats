<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Multiple Linear Regression and Interactions</title>
    <meta charset="utf-8" />
    <meta name="author" content="Tamma Carleton" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Multiple Linear Regression and Interactions
]
.subtitle[
## EDS 222
]
.author[
### Tamma Carleton
]
.date[
### Fall 2022
]

---




# Announcements/check-in

- Thank you for filling out the survey!
  + Lectures: 64% (right on), 36% (too fast)
  + Labs: 96% (right on), 1% (too fast)
  + HW: 84% (right on), 16% (too fast)

--

- In response to survey:
  + Adding more examples of applying concepts _during the lecture_
  + Slightly slower pace in lectures, try to remove jargon where possible, ensure definitions are provided
  + **Key adjustments to HW3**: due Nov 1, 9am; graded Pass/Fail; answer key posted with HW to aid midterm studying
  + Extra example of testing OLS assumptions posted on our [Resources page](https://tcarleton.github.io/EDS-222-stats/resources.html)
  
---
# Midterm Exam

## Two parts:

--

**Part 1: Short answer questions (~4)**

  + Focus on definitions of key concepts
  
  + You should know key definitions (e.g., expectation/mean, median, variance, `\(R^2\)`, OLS slope and intercept formulas for simple linear regression)
  
  + You do not need to memorize math rules (e.g., `\(var(ax+b)=a^2var(x)\)`)
  
  + Be able to interpret probability distributions, scatter plots, QQ-plots, boxplots, linear regression output (not `\(p\)`-values or `\(t\)`-statistics)

---
# Midterm Exam

## Two parts:

**Part 2: Long answer questions (~2)**

  + Each question poses a data science problem and walks you through a set of analysis steps
  
  + Very similar to assignments but focused on interpretation of existing code and output
  
  + May include some minimal pseudo-coding

---
name: Overview

# Today

#### Model fit in multiple regression
Nonlinear relationships in linear models, adjusted `\(R^2\)`

--

#### Interaction effects
Implementation and interpretation

--

#### Multicollinearity
Problems and (some) solutions

---

layout: false
class: clear, middle, inverse
# Model fit in multiple regression

---
# Nonlinear transformations

- Our linearity assumption requires that **parameters enter linearly** (_i.e._, the `\(\beta_k\)` multiplied by variables)
- We allow nonlinear relationships between `\(y\)` and the explanatory variables `\(x\)`.

**Example: Polynomials**

`$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + u_i$$`

`$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^2=3 + u_i$$`

`$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \beta_4 x_i^4 + u_i$$`

...
---
# Polynomials

- Consider the relationship between **temperature** and **harmful algal blooms** (this is a [real thing!](https://www.epa.gov/nutrientpollution/climate-change-and-harmful-algal-blooms#:~:text=Warmer%20temperatures%20prevent%20water%20from,warmer%20and%20promoting%20more%20blooms.)).

- Suppose we sampled many coastal locations across the US, and measured the total surface water area at each site that had blooms present. 

- Perhaps we have scientific evidence to suggest there is a nonlinear effect of temperature on extent of the blooms. 

- We might want to estimate the following model:

$$ area_i = \beta_0 + \beta_1 temperature_i + \beta_2 temperature_i^2 + u_i$$
---
# Polynomials

$$ area_i = \beta_0 + \beta_1 temperature_i + \beta_2 temperature_i^2 + u_i$$

&lt;img src="05-multiplereg_files/figure-html/polys-1.svg" style="display: block; margin: auto;" /&gt;
---
# Polynomials

Estimating polynomial regressions in `R`, option 1:

```r
blooms_df = blooms_df %&gt;% mutate(temp2 = temp^2)
summary(lm(area~temp+temp2, data=blooms_df))
```

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = area ~ temp + temp2, data = blooms_df)
#&gt; 
#&gt; Residuals:
#&gt;      Min       1Q   Median       3Q      Max 
#&gt; -12.5966  -2.0923  -0.1423   1.9951   9.4874 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)  0.06363    0.29249   0.218    0.828    
#&gt; temp         0.62544    0.44007   1.421    0.156    
#&gt; temp2        1.92118    0.14160  13.567   &lt;2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 3.021 on 997 degrees of freedom
#&gt; Multiple R-squared:  0.7772,	Adjusted R-squared:  0.7768 
#&gt; F-statistic:  1739 on 2 and 997 DF,  p-value: &lt; 2.2e-16
```

---
# Polynomials

Estimating polynomial regressions in `R`, option 2:

```r
summary(lm(area~temp+I(temp^2), data=blooms_df))
```

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = area ~ temp + I(temp^2), data = blooms_df)
#&gt; 
#&gt; Residuals:
#&gt;      Min       1Q   Median       3Q      Max 
#&gt; -12.5966  -2.0923  -0.1423   1.9951   9.4874 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)  0.06363    0.29249   0.218    0.828    
#&gt; temp         0.62544    0.44007   1.421    0.156    
#&gt; I(temp^2)    1.92118    0.14160  13.567   &lt;2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 3.021 on 997 degrees of freedom
#&gt; Multiple R-squared:  0.7772,	Adjusted R-squared:  0.7768 
#&gt; F-statistic:  1739 on 2 and 997 DF,  p-value: &lt; 2.2e-16
```
---
# Polynomials

Watch out! Some things are not intuitive:

```r
summary(lm(area~poly(area,2), data=blooms_df))
```

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = area ~ poly(area, 2), data = blooms_df)
#&gt; 
#&gt; Residuals:
#&gt;        Min         1Q     Median         3Q        Max 
#&gt; -1.278e-13 -6.600e-16  7.900e-17  3.970e-16  3.026e-13 
#&gt; 
#&gt; Coefficients:
#&gt;                 Estimate Std. Error   t value Pr(&gt;|t|)    
#&gt; (Intercept)    7.059e+00  3.304e-16 2.136e+16   &lt;2e-16 ***
#&gt; poly(area, 2)1 2.021e+02  1.045e-14 1.934e+16   &lt;2e-16 ***
#&gt; poly(area, 2)2 1.394e-14  1.045e-14 1.334e+00    0.182    
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 1.045e-14 on 997 degrees of freedom
#&gt; Multiple R-squared:      1,	Adjusted R-squared:      1 
#&gt; F-statistic: 1.871e+32 on 2 and 997 DF,  p-value: &lt; 2.2e-16
```

---
# Polynomials

Watch out! Some things are not intuitive (need `raw=TRUE` for coefficients to be interpretable -- see helpful Stack Overflow on this [here](https://stackoverflow.com/questions/19484053/what-does-the-r-function-poly-really-do)):

```r
summary(lm(area~poly(temp,2, raw=TRUE), data=blooms_df))
```

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = area ~ poly(temp, 2, raw = TRUE), data = blooms_df)
#&gt; 
#&gt; Residuals:
#&gt;      Min       1Q   Median       3Q      Max 
#&gt; -12.5966  -2.0923  -0.1423   1.9951   9.4874 
#&gt; 
#&gt; Coefficients:
#&gt;                            Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)                 0.06363    0.29249   0.218    0.828    
#&gt; poly(temp, 2, raw = TRUE)1  0.62544    0.44007   1.421    0.156    
#&gt; poly(temp, 2, raw = TRUE)2  1.92118    0.14160  13.567   &lt;2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 3.021 on 997 degrees of freedom
#&gt; Multiple R-squared:  0.7772,	Adjusted R-squared:  0.7768 
#&gt; F-statistic:  1739 on 2 and 997 DF,  p-value: &lt; 2.2e-16
```

---
# Polynomials

|            |  Estimate| Std. Error|    t value| Pr(&gt;&amp;#124;t&amp;#124;)|
|:-----------|---------:|----------:|----------:|------------------:|
|(Intercept) | 0.0636289|   0.292487|  0.2175444|          0.8278286|
|temp        | 0.6254436|   0.440068|  1.4212430|          0.1555588|
|I(temp^2)   | 1.9211754|   0.141604| 13.5672357|          0.0000000|

--

How do we interpret these coefficients?

- Intercept: Predicted area of bloom when temperature = 0 degrees C

--

- `\(\hat\beta_1\)` (coeff. on `\(temp\)`) and `\(\hat\beta_2\)` (coeff. on `\(temp^2\)`)... ???  

--

Go back to Algebra II (see [here](http://www.abcte.org/files/previews/math/s4_p2.html) for a refresher): `\(y=ax^2 + bx + c\)`. `\(a\)` tells you whether the U-shape faces up or down, and how narrow or wide it is; `\(b\)` tells you whether the U-shape shifts left or right away from the `\(y\)`-axis; `\(c\)` simply shifts the U-shape up or down.

---
# Polynomials

**Don't worry about the Algebra II if it doesn't feel familiar!**

$$ area_i = \beta_0 + \beta_1 temperature_i + \beta_2 temperature_i^2 + u_i$$

You can always:
- Graph your predicted values using `geom_smooth()` (see Lab 5)
- Put your coefficients into an automated grapher function (online or on your Mac)
- Use the regression output directly, along with a little basic math (e.g., predict area at temperature = 15, then at temperature = 16, and take the difference!)

--

**Key insight: effect of an increase in temperature on algal bloom area depends on the baseline level of temperature!** (true for all nonlinear relationships)
---
# Nonlinear transformations

Other examples:

- **Polynomials** and **interactions:** `\(y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{1i}^2 + \beta_3 x_{2i} + \beta_4 x_{2i}^2 + \beta_5 \left( x_{1i} x_{2i} \right) + u_i\)`

- **Exponentials** and **logs:** `\(\log(y_i) = \beta_0 + \beta_1 x_{1i} + \beta_2 e^{x_{2i}} + u_i\)` (more on this next week)

- **Indicators** and **thresholds:** `\(y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 \, \mathbb{I}(x_{1i} \geq 100) + u_i\)`

--

In all cases, the effect of a change in `\(x\)` on `\(y\)` will vary depending on your baseline level of `\(x\)`. This is not true with linear relationships! 

---
# Nonlinear transformations

**Transformation challenge:** (literally) infinite possibilities. What do we pick?

&lt;img src="05-multiplereg_files/figure-html/transfigurestart-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false
# Nonlinear transformations

`\(y_i = \beta_0 + u_i\)`

&lt;img src="05-multiplereg_files/figure-html/transfigure0-1.svg" style="display: block; margin: auto;" /&gt;
---
count: false
# Nonlinear transformations

`\(y_i = \beta_0 + \beta_1 x + u_i\)`

&lt;img src="05-multiplereg_files/figure-html/transfigure1-1.svg" style="display: block; margin: auto;" /&gt;
---
count: false
# Nonlinear transformations

`\(y_i = \beta_0 + \beta_1 x + \beta_2 x^2 + u_i\)`

&lt;img src="05-multiplereg_files/figure-html/transfigure2-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false
# Nonlinear transformations

`\(y_i = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + u_i\)`

&lt;img src="05-multiplereg_files/figure-html/transfigure3-1.svg" style="display: block; margin: auto;" /&gt;
---
count: false
# Nonlinear transformations

`\(y_i = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + u_i\)`

&lt;img src="05-multiplereg_files/figure-html/transfigure4-1.svg" style="display: block; margin: auto;" /&gt;
---
count: false
# Nonlinear transformations


`\(y_i = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + u_i\)`

&lt;img src="05-multiplereg_files/figure-html/transfigure5-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false
# Nonlinear transformations


**Truth:** `\(y_i = 2 e^{x} + u_i\)`

&lt;img src="05-multiplereg_files/figure-html/trans figure 6-1.svg" style="display: block; margin: auto;" /&gt;

---
# Model fit with multiple regressors

Measures of *goodness of fit* try to analyze how well our model describes (*fits*) the data.

--

**Common measure:** `\(R^2\)` [R-squared] (*a.k.a.* coefficient of determination)

$$ R^2 =  1 - \dfrac{\sum_i \left( y_i - \hat{y}_i \right)^2}{\sum_i \left( y_i - \overline{y} \right)^2} =  1 - \dfrac{\sum_i e_i^2}{\sum_i \left( y_i - \overline{y} \right)^2} $$

Recall `\(\sum_i \left( y_i - \hat{y}_i \right)^2 = \sum_i e_i^2\)` is the "sum of squared errors".

--

`\(R^2\)` literally tells us the share of the variance in `\(y\)` our current models accounts for. Thus `\(0 \leq R^2 \leq 1\)`.

---
# Model fit with multiple regressors

**The problem:** As we add variables to our model, `\(R^2\)` *mechanically* increases.

--

**Intuition:** Even if our added variable has _no true relation to_ `\(y\)`, it can help lower `\(e_i\)` by fitting to the sampling noise

--


**One solution:**
Penalize for the number of variables, _e.g._, .pink[adjusted] `\(\color{#e64173}{R^2}\)`:

$$ \overline{R}^2 = 1 - \dfrac{\sum_i \left( y_i - \hat{y}_i \right)^2/(n-k-1)}{\sum_i \left( y_i - \overline{y} \right)^2/(n-1)} $$

*Note:* Adjusted `\(R^2\)` need not be between 0 and 1.

---

# Model fit with multiple regressors

#### We often use measures of model fit (or model "performance") to help choose a regression model from among multiple possibilities

- Adjusted `\(R^2\)` is just one of **many possible performance metrics**

--

- For example, Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), Mean Squared Error (MSE), ...

--

- Lots more on the topic of model selection in EDS 232 ðŸ‘€

--

- Don't forget the _theory_ behind your data science! 

---
layout: false
class: clear, middle, inverse
# Interactions
---
# Interactions

Interactions allow the effect of one variable to change based upon the level of another variable.

**Examples**

1. Does the effect of schooling on pay change by gender?

2. Does the effect of gender on pay change by race?

3. Does the effect of schooling on pay change by experience?

4. ??

---
# Interactions

Previously, we considered a model that allowed women and men to have different average wages, but the model assumed the effect of schooling on pay was the same for everyone:

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + \beta_2 \, \text{Male}_i + u_i $$

but we can also allow the effect of school to vary by gender:

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + \beta_2 \, \text{Male}_i + \beta_3 \, \text{School}_i\times\text{Male}_i + u_i $$

--

The multiplication of `\(School\)` by `\(Male\)` is called an **interaction term** 

---
# Interactions

The model where schooling has the same effect for everyone (**&lt;font color="#e64173"&gt;F&lt;/font&gt;** and **&lt;font color="#314f4f"&gt;M&lt;/font&gt;**):

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + \beta_2 \, \text{Male}_i + u_i $$



&lt;img src="05-multiplereg_files/figure-html/intplot1-1.svg" style="display: block; margin: auto;" /&gt;

---
# Interactions

The model where schooling's effect can differ by gender (**&lt;font color="#e64173"&gt;F&lt;/font&gt;** and **&lt;font color="#314f4f"&gt;M&lt;/font&gt;**):

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + \beta_2 \, \text{Male}_i + \beta_3 \, \text{School}_i\times\text{Male}_i + u_i $$

&lt;img src="05-multiplereg_files/figure-html/intplot2-1.svg" style="display: block; margin: auto;" /&gt;

---
# Interactions

Interpreting coefficients can be a little tricky -- carefully working through the math helps.

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + \beta_2 \, \text{Male}_i + \beta_3 \, \text{School}_i\times\text{Male}_i + u_i $$
Expected effect of an additional year of schooling for **women**:

$$
`\begin{aligned}
\text{Pay}_i(\text{Female}\hskip1mm,\text{School}_i = \ell + 1)  -
    \text{Pay}_i(\text{Female}\hskip1mm,\text{School}_i = \ell) &amp;= \\
 (\beta_0 + \beta_1 *(\ell+1) + \beta_2*0 + \beta_3 *(\ell+1)*0 + u_i) &amp;- \\
    (\beta_0 + \beta_1 * \ell + \beta_2*0 + \beta_3 *\ell*0 + u_i ) &amp;= \\
     \beta_1 *(\ell+1) - \beta_1 * \ell &amp;= \\
 \beta_1 
\end{aligned}`
$$

---
# Interactions

Interpreting coefficients can be a little tricky -- carefully working through the math helps.

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + \beta_2 \, \text{Male}_i + \beta_3 \, \text{School}_i\times\text{Male}_i + u_i $$

Expected returns for an additional year of schooling for **men**:

$$
`\begin{aligned}
\text{Pay}_i(\text{Male}\hskip1mm,\text{School}_i = \ell + 1)  -
    \text{Pay}_i(\text{Male}\hskip1mm,\text{School}_i = \ell) &amp;= \\
 (\beta_0 + \beta_1 *(\ell+1) + \beta_2 + \beta_3 *(\ell+1) + u_i) &amp;- \\
    (\beta_0 + \beta_1 * \ell + \beta_2 + \beta_3 *\ell + u_i ) &amp;= \\
     \beta_1 *(\ell+1) - \beta_1 * \ell +  \beta_3 *(\ell+1) - \beta_3 * \ell &amp;= \\
 \beta_1 + \beta_3 
\end{aligned}`
$$

--

Thus, `\(\beta_3\)` gives the **difference in the returns to schooling** for men versus women.

---
# Interactions

$$ \text{Pay}_i = \beta_0 + \beta_1 \, \text{School}_i + \beta_2 \, \text{Male}_i + \beta_3 \, \text{School}_i\times\text{Male}_i + u_i $$

Another way to do interpretation: rearrange the expression. 

--

- Effect of one more year of schooling on expected pay:

$$ Pay_i = \beta_0 + \beta_2 Male_i + (\beta_1 + \beta_3 Male_i)\times School_i + u_i $$
This helps you see that the effect of `\(School\)` on `\(Pay\)` is `\(\beta_1 + \beta_3 Male\)`, so it will vary based on whether an individual is `\(Male\)` or `\(Female\)`

---


# Interactions

$$ {y\_i} = \beta\_0 + \beta\_1 {x\_{1i}} + \beta\_2 {x\_{2i}} + \beta\_3 x\_{1i}\times x\_{2i} + u\_i $$

In general, interaction models should be used when **the level of one variable influences the relationship between the outcome and another variables**

--

For example:

- Income changes the relationship between extreme heat and mortality (Carleton et al., 2022)

--

- Gender changes the relationship between air pollution and labor productivity (Graff-Zivin and Neidell, 2021)

--

- Other examples?

---
# Interactions

$$ {y\_i} = \beta\_0 + \beta\_1 {x\_{1i}} + \beta\_2 {x\_{2i}} + \beta\_3 x\_{1i}\times x\_{2i} + u\_i $$


#### Interpreting interaction models means you have to consider the interaction term when computing slopes. 

For example: What is the "slope" of the relationship between `\(y\)` and `\(x_1\)`?

--

$$
`\begin{aligned}
 y_i(x_{i2}, x_{i1} = \ell + 1) -
     y_i (x_{i2}, x_{i1} = \ell ) &amp;= \\
 (\beta_0 + \beta_1 *(\ell+1) + \beta_2 x_{i2} + \beta_3* (\ell+1) \times x_{i2} + u_i ) &amp;- \\
    (\beta_0 + \beta_1 * \ell + \beta_2 x_{i2} + \beta_3 * \ell \times x_{i2} + u_i  ) &amp;= \\
 \beta_1 + \beta_3 x_{i2}
\end{aligned}`
$$
**Key insight: Higher `\(x_{i2}\)` increases the slope of the relationship between `\(y\)` and `\(x_1\)`!** The inverse is also true.

For two continuous random variables, we now have infinitely many slopes for each variable, depending on the level of the other independent variable.

---
# Interactions 

Putting it all in one place...interaction models with two continuous variables:

$$ {y\_i} = \beta\_0 + \beta\_1 {x\_{1i}} + \beta\_2 {x\_{2i}} + \beta\_3 x\_{1i}\times x\_{2i} + u\_i $$

--

- `\(\beta_3\)` is the **difference** in the effect of `\(x_1\)` on `\(y\)` between an individual with `\(x_2=\ell+1\)` and an individual with `\(x_2=\ell\)`

--
 
- `\(\beta_3\)` is **also** the difference in the effect of `\(x_2\)` on `\(y\)` between an individual with `\(x_1=\ell+1\)` and an individual with `\(x_1=\ell\)`

--
 
- `\(\beta_0\)` is the predicted level of `\(y\)` when **both** `\(x_1\)` and `\(x_2\)` are zero

--
 
- `\(\beta_1\)` is the effect of `\(x_1\)` on `\(y\)` when `\(x_2\)` is zero

--
 
- `\(\beta_2\)` is the effect of `\(x_2\)` on `\(y\)` when `\(x_1\)` is zero

---
# Interactions in R

This will be the focus of Lab on Thursday.

As a preview...just like many other aspects of regression analysis, interactions are easy to implement but difficult to carefully interpret in `R`:

--


```r
summary(lm(hwy ~ displ + year + displ:year, data = mpg))
```

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = hwy ~ displ + year + displ:year, data = mpg)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -7.8595 -2.4360 -0.2103  1.6037 15.3677 
#&gt; 
#&gt; Coefficients:
#&gt;              Estimate Std. Error t value Pr(&gt;|t|)
#&gt; (Intercept) -40.72485  319.45688  -0.127    0.899
#&gt; displ       -71.54962   86.41661  -0.828    0.409
#&gt; year          0.03828    0.15947   0.240    0.811
#&gt; displ:year    0.03391    0.04313   0.786    0.433
#&gt; 
#&gt; Residual standard error: 3.784 on 230 degrees of freedom
#&gt; Multiple R-squared:  0.6015,	Adjusted R-squared:  0.5963 
#&gt; F-statistic: 115.7 on 3 and 230 DF,  p-value: &lt; 2.2e-16
```

---
layout: false
class: clear, middle, inverse
# Multicollinearity

---
# Multicollinearity

$$ {y\_i} = \beta\_0 + \beta\_1 {x\_{1i}} + \beta\_2 {x\_{2i}} + \cdots + \beta\_k {x\_{ki}} + u\_i $$
### What is it?

- When 2 (_collinearity_) or more (_multicollinearity_) of your independent variables are highly correlated with one another

--

### What is the problem?

- Coefficients change _substantially_ with small changes in independent variables  
- Illogical/unexpected coefficients

---
# Multicollinearity

### Why might it happen?

- Too many independent variables ("overspecified" model)
- Including dummy variable for your reference group 
- True population correlation between variables is high


---
# Multicollinearity

#### Easy check: `ggpairs()`, `pairs()`, etc.

&lt;img src="collinearity.png" width="60%" style="display: block; margin: auto;" /&gt;

---
# Multicollinearity

### What to do about it?

- More data helps, if possible

- Check if some variables should be omitted based on theory/conceptual model (e.g., reference group dummy)?

- Eliminate highly correlated variables (ensure your interpretation changes accordingly)
  + E.g., temperature and humidity

---

class: center, middle


Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

Some slides and slide components were borrowed from [Ed Rubin's](https://github.com/edrubin/EC421S20) awesome course materials.


&lt;style type="text/css"&gt;
@media print {
  .has-continuation {
    display: block;
  }
}
&lt;/style&gt;
  
---
exclude: true




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
